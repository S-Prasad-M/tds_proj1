{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "#### Who are the top 5 users in Delhi with the highest number of followers? List their login in order, comma-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amitshekhariitbhu,shradha-khapra,loveBabbar,Nakshatra05,Anuj-Kumar-Sharma\n"
     ]
    }
   ],
   "source": [
    "users_in_delhi = []\n",
    "with open('users.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        location = row['location'].strip().lower()\n",
    "        if 'delhi' in location:\n",
    "            users_in_delhi.append({\n",
    "                'login': row['login'],\n",
    "                'followers': int(row['followers'])\n",
    "            })\n",
    "\n",
    "top_users = sorted(users_in_delhi, key=lambda x: x['followers'], reverse=True)\n",
    "top_5_logins = [user['login'] for user in top_users[:5]]\n",
    "print(','.join(top_5_logins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "#### Who are the 5 earliest registered GitHub users in Delhi? List their login in ascending order of created_at, comma-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-aalam,vaidik,dineshkummarc,dash1291,DroidNinja\n"
     ]
    }
   ],
   "source": [
    "users_in_delhi = []\n",
    "with open('users.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        location = row['location'].strip().lower()\n",
    "        # Check if the user is from Delhi\n",
    "        if 'delhi' in location:\n",
    "            users_in_delhi.append({\n",
    "                'login': row['login'],\n",
    "                'created_at': datetime.strptime(row['created_at'], '%Y-%m-%dT%H:%M:%SZ')\n",
    "            })\n",
    "\n",
    "sorted_users = sorted(users_in_delhi, key=lambda x: x['created_at'])\n",
    "top_5_earliest_logins = [user['login'] for user in sorted_users[:5]]\n",
    "print(','.join(top_5_earliest_logins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "#### What are the 3 most popular license among these users? Ignore missing licenses. List the license_name in order, comma-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mit,apache-2.0,other\n"
     ]
    }
   ],
   "source": [
    "licenses = []\n",
    "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        # Check if the license_name field is present and not empty\n",
    "        license_name = row.get('license_name', '').strip()\n",
    "        if license_name:\n",
    "            licenses.append(license_name)\n",
    "\n",
    "license_counts = Counter(licenses)\n",
    "top_3_licenses = [license for license, count in license_counts.most_common(3)]\n",
    "print(','.join(top_3_licenses))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "#### Which company do the majority of these developers work at?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASAI SCHOOL\n"
     ]
    }
   ],
   "source": [
    "companies = []\n",
    "with open('users.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        company = row.get('company', '').strip()\n",
    "        if company:\n",
    "            companies.append(company)\n",
    "\n",
    "company_counts = Counter(companies)\n",
    "most_common_company = company_counts.most_common(1)\n",
    "if most_common_company:\n",
    "    print(most_common_company[0][0])\n",
    "else:\n",
    "    print(\"No company data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "#### Which programming language is most popular among these users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JavaScript\n"
     ]
    }
   ],
   "source": [
    "languages = []\n",
    "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        language = row.get('language', '').strip()\n",
    "        if language:\n",
    "            languages.append(language)\n",
    "\n",
    "language_counts = Counter(languages)\n",
    "most_common_language = language_counts.most_common(1)\n",
    "if most_common_language:\n",
    "    print(most_common_language[0][0])\n",
    "else:\n",
    "    print(\"No language data found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "#### Which programming language is the second most popular among users who joined after 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML\n"
     ]
    }
   ],
   "source": [
    "languages = []\n",
    "\n",
    "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        created_at = row.get('created_at', '').strip()\n",
    "        if created_at:\n",
    "            user_join_date = datetime.strptime(created_at, \"%Y-%m-%dT%H:%M:%SZ\")\n",
    "            if user_join_date.year > 2020:\n",
    "                language = row.get('language', '').strip()\n",
    "                if language:\n",
    "                    languages.append(language)\n",
    "\n",
    "language_counts = Counter(languages)\n",
    "most_common_languages = language_counts.most_common(2)\n",
    "if len(most_common_languages) >= 2:\n",
    "    print(most_common_languages[1][0])  # Second most common language\n",
    "else:\n",
    "    print(\"Not enough language data found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "####  Which language has the highest average number of stars per repository?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Svelte\n"
     ]
    }
   ],
   "source": [
    "language_stats = defaultdict(lambda: {'stars': 0, 'repos': 0})\n",
    "\n",
    "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        language = row.get('language', '').strip()\n",
    "        stars = row.get('stargazers_count', '0').strip()\n",
    "        if language and stars.isdigit():\n",
    "            language_stats[language]['stars'] += int(stars)\n",
    "            language_stats[language]['repos'] += 1\n",
    "\n",
    "average_stars_per_language = {\n",
    "    language: stats['stars'] / stats['repos']\n",
    "    for language, stats in language_stats.items()\n",
    "    if stats['repos'] > 0\n",
    "}\n",
    "if average_stars_per_language:\n",
    "    most_popular_language = max(average_stars_per_language, key=average_stars_per_language.get)\n",
    "    print(most_popular_language)\n",
    "else:\n",
    "    print(\"No language data found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "####  Let's define leader_strength as followers / (1 + following). Who are the top 5 in terms of leader_strength? List their login in order, comma-separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anuj-Kumar-Sharma,Ignitetechnologies,shradha-khapra,loveBabbar,amitshekhariitbhu\n"
     ]
    }
   ],
   "source": [
    "leader_strengths = []\n",
    "\n",
    "with open('users.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)    \n",
    "    for row in reader:\n",
    "        followers = int(row.get('followers', 0).strip())\n",
    "        following = int(row.get('following', 0).strip())\n",
    "        leader_strength = followers / (1 + following)\n",
    "        leader_strengths.append((row.get('login', ''), leader_strength))\n",
    "\n",
    "leader_strengths.sort(key=lambda x: x[1], reverse=True)\n",
    "top_5_leaders = [login for login, strength in leader_strengths[:5]]\n",
    "print(','.join(top_5_leaders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "#### What is the correlation between the number of followers and the number of public repositories among users in Delhi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.017\n"
     ]
    }
   ],
   "source": [
    "followers = []\n",
    "public_repos = []\n",
    "\n",
    "with open('users.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)    \n",
    "    for row in reader:\n",
    "        location = row.get('location', '').strip().lower()\n",
    "        if \"delhi\" in location:\n",
    "            try:\n",
    "                followers_count = int(row['followers'])\n",
    "                public_repos_count = int(row['public_repos'])\n",
    "                followers.append(followers_count)\n",
    "                public_repos.append(public_repos_count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "if len(followers) > 1 and len(public_repos) > 1:\n",
    "    correlation_matrix = np.corrcoef(followers, public_repos)\n",
    "    correlation = correlation_matrix[0, 1]\n",
    "    print(f\"{correlation:.3f}\")\n",
    "else:\n",
    "    print(\"Insufficient data for correlation calculation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "#### Does creating more repos help users get more followers? Using regression, estimate how many additional followers a user gets per additional public repository. <i>{Regression slope of followers on repos (to 3 decimal places, e.g. 0.123 or -0.123)}</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.048\n"
     ]
    }
   ],
   "source": [
    "followers = []\n",
    "public_repos = []\n",
    "with open('users.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        location = row.get('location', '').strip().lower()\n",
    "        if \"delhi\" in location:\n",
    "            try:\n",
    "                followers_count = int(row['followers'])\n",
    "                public_repos_count = int(row['public_repos'])\n",
    "                followers.append(followers_count)\n",
    "                public_repos.append(public_repos_count)\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "if len(followers) > 1 and len(public_repos) > 1:\n",
    "    slope, intercept = np.polyfit(public_repos, followers, 1)\n",
    "    print(f\"{slope:.3f}\")\n",
    "else:\n",
    "    print(\"Insufficient data for regression.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "#### Do people typically enable projects and wikis together? What is the correlation between a repo having projects enabled and having wiki enabled? <i>Correlation between projects and wiki enabled (to 3 decimal places, e.g. 0.123 or -0.123)<\\i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation coefficient: 0.16\n",
      "\n",
      "Additional Statistics:\n",
      "total_repos: 30075\n",
      "projects_enabled: 29988\n",
      "wiki_enabled: 27549\n",
      "both_enabled: 27541\n",
      "neither_enabled: 79\n"
     ]
    }
   ],
   "source": [
    "def analyze_repo_features(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    if df['has_projects'].dtype == 'object':\n",
    "        df['has_projects'] = df['has_projects'].map({'true': True, 'false': False})\n",
    "    if df['has_wiki'].dtype == 'object':\n",
    "        df['has_wiki'] = df['has_wiki'].map({'true': True, 'false': False})\n",
    "    \n",
    "    correlation = df['has_projects'].corr(df['has_wiki'])\n",
    "    stats = {\n",
    "        'total_repos': len(df),\n",
    "        'projects_enabled': df['has_projects'].sum(),\n",
    "        'wiki_enabled': df['has_wiki'].sum(),\n",
    "        'both_enabled': ((df['has_projects']) & (df['has_wiki'])).sum(),\n",
    "        'neither_enabled': ((~df['has_projects']) & (~df['has_wiki'])).sum()\n",
    "    }\n",
    "    return round(correlation, 3), stats\n",
    "\n",
    "correlation, stats = analyze_repo_features('repositories.csv')\n",
    "print(f\"Correlation coefficient: {correlation}\")\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "#### Do hireable users follow more people than those who are not hireable? <i>Average of following per user for hireable=true minus the average following for the rest (to 3 decimal places, e.g. 12.345 or -12.345)</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-175.028"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"users.csv\")\n",
    "hireable_following = df[df['hireable'] == True]['following'].mean()\n",
    "non_hireable_following = df[df['hireable'] != True]['following'].mean()\n",
    "difference = round(hireable_following - non_hireable_following, 3)\n",
    "difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 13\n",
    "#### Some developers write long bios. Does that help them get more followers? What's the impact of the length of their bio (in Unicode words, split by whitespace) with followers? (Ignore people without bios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Regression slope of followers on bio word count: 11.883\n"
     ]
    }
   ],
   "source": [
    "csv_file = 'users.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "# print(\"DataFrame Overview:\")\n",
    "# print(df.head())\n",
    "# print(\"\\nDataFrame Info:\")\n",
    "# print(df.info())\n",
    "df = df[df['bio'].notnull()]\n",
    "df['bio_word_count'] = df['bio'].str.split().str.len()\n",
    "X = df['bio_word_count']\n",
    "y = df['followers']\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "slope = model.params['bio_word_count']\n",
    "print(f\"\\nRegression slope of followers on bio word count: {slope:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "#### Who created the most repositories on weekends (UTC)? List the top 5 users' login in order, comma-separated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dheeraj-thedev,coding-blocks-archives,Ayush7614,imvickykumar999,dineshkummarc\n"
     ]
    }
   ],
   "source": [
    "weekend_repo_counts = Counter()\n",
    "with open('repositories.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        created_at = row.get('created_at', '')\n",
    "        if created_at:\n",
    "            created_date = datetime.fromisoformat(created_at[:-1])\n",
    "            if created_date.weekday() in [5, 6]:\n",
    "                user_login = row['login']\n",
    "                weekend_repo_counts[user_login] += 1 \n",
    "\n",
    "top_users = weekend_repo_counts.most_common(5)\n",
    "top_logins = [user[0] for user in top_users]\n",
    "print(','.join(top_logins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "#### Do people who are hireable share their email addresses more often?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total users: 425\n",
      "Hireable users with email: 101/162\n",
      "Non-hireable users with email: 97/263\n",
      "Hireable fraction: 0.623\n",
      "Non-hireable fraction: 0.369\n",
      "\n",
      "Final result: 0.255\n"
     ]
    }
   ],
   "source": [
    "def analyze_email_sharing(users_csv_path='users.csv'):\n",
    "    # Read the complete CSV file\n",
    "    df = pd.read_csv(users_csv_path)\n",
    "    \n",
    "    # Convert email column to boolean (True if email exists, False if NaN or empty)\n",
    "    df['has_email'] = df['email'].notna() & (df['email'] != '')\n",
    "    \n",
    "    # Calculate for hireable users\n",
    "    hireable_mask = df['hireable'] == True\n",
    "    if hireable_mask.any():\n",
    "        hireable_email_fraction = df[hireable_mask]['has_email'].mean()\n",
    "    else:\n",
    "        hireable_email_fraction = 0\n",
    "        \n",
    "    # Calculate for non-hireable users\n",
    "    non_hireable_mask = df['hireable'] != True\n",
    "    if non_hireable_mask.any():\n",
    "        non_hireable_email_fraction = df[non_hireable_mask]['has_email'].mean()\n",
    "    else:\n",
    "        non_hireable_email_fraction = 0\n",
    "    difference = round(hireable_email_fraction - non_hireable_email_fraction, 3)\n",
    "    print(f\"Total users: {len(df)}\")\n",
    "    print(f\"Hireable users with email: {df[hireable_mask]['has_email'].sum()}/{hireable_mask.sum()}\")\n",
    "    print(f\"Non-hireable users with email: {df[non_hireable_mask]['has_email'].sum()}/{non_hireable_mask.sum()}\")\n",
    "    print(f\"Hireable fraction: {hireable_email_fraction:.3f}\")\n",
    "    print(f\"Non-hireable fraction: {non_hireable_email_fraction:.3f}\")\n",
    "    \n",
    "    return difference\n",
    "result = analyze_email_sharing()\n",
    "print(f\"\\nFinal result: {result:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16\n",
    "#### Let's assume that the last word in a user's name is their surname (ignore missing names, trim and split by whitespace.) What's the most common surname? (If there's a tie, list them all, comma-separated, alphabetically)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singh: 22\n"
     ]
    }
   ],
   "source": [
    "surname_counter = Counter()\n",
    "with open('users.csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        name = row.get('name', '').strip()\n",
    "        if name: \n",
    "            surname = name.split()[-1]\n",
    "            surname_counter[surname] += 1\n",
    "\n",
    "if surname_counter:\n",
    "    max_count = max(surname_counter.values())\n",
    "    most_common_surnames = [surname for surname, count in surname_counter.items() if count == max_count]\n",
    "    most_common_surnames.sort()\n",
    "    print(f\"{', '.join(most_common_surnames)}: {max_count}\")\n",
    "else:\n",
    "    print(\"No names found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
